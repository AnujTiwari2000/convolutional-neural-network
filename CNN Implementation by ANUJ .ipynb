{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ef874c7",
   "metadata": {},
   "source": [
    "# Implementation of CNN and Comparision with ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becb0668",
   "metadata": {},
   "source": [
    "Import the required libraies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1efd92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets,layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a863f84d",
   "metadata": {},
   "source": [
    "**Load the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6550167c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "X_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1763fa9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc1c8da8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280eb80f",
   "metadata": {},
   "source": [
    "Let's plot some images to see what they are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8de6d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample(X, y, index):\n",
    "    plt.figure(figsize = (15,3))\n",
    "    plt.imshow(X[231])\n",
    "    plt.xlabel([y[231]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "465b804d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plot_sample(X_train, y_train, 235)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c630b7",
   "metadata": {},
   "source": [
    "Normalize the images to a number from 0 to 1. Image has 3 channels (R,G,B) and each value in the channel can range from 0 to 255. Hence to normalize in 0-->1 range, we need to divide it by 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb95397d",
   "metadata": {},
   "source": [
    " **Normalisation of training dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "447eddfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c5a803",
   "metadata": {},
   "source": [
    "**simple artificial neural network Model for image classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0441ac51",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tf.expand_dims(X_train, axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46eb2415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1875/1875 [==============================] - 44s 23ms/step - loss: 0.4999 - accuracy: 0.8783\n",
      "Epoch 2/2\n",
      "1875/1875 [==============================] - 43s 23ms/step - loss: 0.2513 - accuracy: 0.9295\n"
     ]
    }
   ],
   "source": [
    "ann = models.Sequential([\n",
    "        layers.Flatten(input_shape=(28,28,1)),\n",
    "        layers.Dense(3000, activation='relu'),\n",
    "        layers.Dense(1000, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')    \n",
    "    ])\n",
    "\n",
    "ann.compile(optimizer='SGD',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "hist=ann.fit(X_train, y_train, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e2a1d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = hist.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba83774f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.4999145269393921, 0.25129613280296326],\n",
       " 'accuracy': [0.8782833218574524, 0.9295499920845032]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cbb87f",
   "metadata": {},
   "source": [
    "**convolutional neural network for image classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d68beca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cnn = models.Sequential([\n",
    "    layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2900ffb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1875/1875 [==============================] - 32s 17ms/step - loss: 0.1389 - accuracy: 0.9576\n",
      "Epoch 2/2\n",
      "1875/1875 [==============================] - 33s 17ms/step - loss: 0.0471 - accuracy: 0.98540s\n"
     ]
    }
   ],
   "source": [
    "cnn.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "hist1=cnn.fit(X_train, y_train, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d799a18",
   "metadata": {},
   "source": [
    "**With CNN, at the end 2 epochs, accuracy was at around 98% which is a significant improvement over ANN. CNN's are best for image classification and gives superb accuracy.\n",
    "Also computation time is almost half as compared to ANN as maxpooling reduces the image dimensions while still preserving the features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "832d4531",
   "metadata": {},
   "outputs": [],
   "source": [
    "history1 = hist1.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d7bf4847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.13890132308006287, 0.04714057594537735],\n",
       " 'accuracy': [0.9576333165168762, 0.9853833317756653]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be5bce7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
